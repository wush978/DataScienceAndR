- Class: meta
  Course: DataScienceAndR
  Lesson: Optional-RMachineLearning-02-Generalized-Linear-Model
  Author: Wush Wu
  Type: Standard
  Organization: Taiwan R User Group
  Version: 2.3.1.2

- Class: text
  Output: |
    這個課程將介紹R 在廣義線性模型上的實作。
    
- Class: text
  Output: |
    傳統的線性迴歸分析中，目標變數通常是沒有範圍限制的數值型變數。當目標變數
    是非負整數（例：來客數）、或是類別型變數（例：有無點擊）時，我們可以運用
    廣義線性模型來做學習。

- Class: cmd_question
  Output: |
    請同學先輸入：`?glm`打開廣義非線性模型的說明頁面。

- Class: text
  Output: |
    glm 的參數和lm很接近，主要都是使用formula與data。formula接受formula物件，而
    data接受data.frame物件。比較不同的是family參數。

- Class: mult_question
  Output: |
    請問glm的family參數預設是？
  AnswerChoices: gaussian;"gaussian";NULL
  CorrectAnswer: "gaussian"
  AnswerTests: omnitest(correctVal = "gaussian")

- Class: cmd_question
  Output: |
    當family是gaussian時，glm的行為和lm一樣。同學可以輸入：`glm(dist ~ speed, data = cars)`看看。

- Class: text
  Output: |
    但是當我們要作分類問題時，family就要做更改。

- Class: cmd_question
  Output: |
    請同學輸入`skip()`檢查mlbench套件
  CorrectAnswer: check_then_install("mlbench", "2.1.1")
  AnswerTests: test_package_version("mlbench", "2.1.1")

- Class: cmd_question
  Output: |
    請同學載入mlbench套件
  CorrectAnswer: library(mlbench)
  AnswerTests: test_search_path("mlbench")

- Class: cmd_question
  Output: |
    請同學輸入：`data(Ionosphere)`載入一個物理實驗的資料集

- Class: text
  Output: |
    上一個單元，我們使用AIC來比較模型的好壞。這個單元，我們把資料分成training和testing
    資料集合，並用模型在testing dataset上的表現做比較。

- Class: text
  Output: |
    同學可以用sample函數來自1:351中抽取testing dataset的列序號。舉例來說，`test.i`中
    就是一組用sample產生的列序號。`train.i`則是運用setdiff產生的列序號。

- Class: cmd_question
  Output: |
    請同學輸入：`df.train <- Ionosphere[train.i,]`建立training dataset的data.frame

- Class: cmd_question
  Output: |
    請同學輸入：`df.test <- Ionosphere[test.i,]`建立testing dataset的data.frame

- Class: cmd_question
  Output: |
    我們先觀察資料集。請同學輸入：`summary(df.train)`
  
- Class: cmd_question
  Output: |
    同學有沒有注意到，有一個欄位的值，沒有變化。請同學以字串形式輸入該欄位的名稱。
  CorrectAnswer: |
    "V2"
  AnswerTests: omnitest(correctVal = "V2")

- Class: cmd_question
  Output: |
    因此，我們模型中就不考慮"V2"。我們只考慮"V1", "V3" ~ "V34"當成解釋變數。
    請同學輸入：`x.name <- setdiff(paste0("V", 1:34), "V2")`來產生一個代表所
    有要放到模型中的解釋變數

- Class: cmd_question
  Output: |
    接著，我們輸入：`f <- reformulate(x.name, "Class")`來建立我們需要的formula物件。
    
- Class: cmd_question
  Output: |
    接著，請同學輸入：`m1 <- glm(formula = f, family = "binomial", data = df.train)`
    建立一個logistic regression的物件。

- Class: text
  Output: |
    請有興趣的同學記住：如果要作兩個類別的分類問題，使用glm時，family = "binomial"就等價於
    logistic regression。

- Class: text
  Output: |
    螢幕上出現的："glm.fit: fitted probabilities numerically 0 or 1 occurred"是因為logistic
    regression在某些類別組合中目標變數全部是相同類別時，會有數值問題。這邊我們先略過，等到
    下一個單元再給一個解決辦法。

- Class: cmd_question
  Output: |
    運用`p1.train <- predict(m1, df.train, type = "response")`就可以算出模型m1在df.train上的預測結果。

- Class: text
  Output: |
    我們真正使用的函數，其實是`predict.glm`，所以有興趣的同學可以打開predict.glm去深入理解
    這個函數的威能。

- Class: cmd_question
  Output: |
    請同學輸入：`plot(Class ~ p1.train, data = df.train)`看一看預測結果與實際結果在training dataset
    上的比較。
    
- Class: text
  Output: |
    我們可以看到，隨著p1.train的值越來越大，good所佔有的比率越來越高。而當p1.train < 0.1時，
    所有的資料都是bad；當p1.train超過某個超過0.7的值時，所有的資料都是good。我們判斷，m1
    是有學到training dataset上的趨勢。

- Class: cmd_question
  Output: |
    接著我們看看testing dataset上的表現。請同學輸入：
    `p1 <- predict(m1, df.test, type = "response")`

- Class: cmd_question
  Output: |
    請同學輸入：`plot(Class ~ p1, df.test)`看一看預測結果在df.test上的表現。

- Class: text
  Output: |
    我們可以看到，當預測機率為0 ~ 0.2時，結果全部都是"bad"。但是當預測結果超過0.8
    時，"good"的比率大致上是0.1左右。由圖判斷，我們學出來的模型在testing dataset上
    是有道理，真的有學到判斷bad/good的方法。

- Class: cmd_question
  Output: |
    運用上一個單元學到的知識，我們可以利用交互作用將模型複雜化。formula的處理，我
    是將 <http://stackoverflow.com/a/29691154/1182304> 中寫的函數，匯入到課程
    環境中給同學使用。他們並不是內建的。請同學輸入：`f2 <- interact_rhs(f)`

- Class: cmd_question
  Output: |
    我們看看`f2`的結果是不是列出解釋變數的所有二次交互作用

- Class: cmd_question
  Output: |
    接著，請輸入：`m2 <- glm(formula = f2, family = "binomial", data = df.train)`
    用更複雜的模型來作學習。

- Class: text
  Output: |
    事實上這個模型已經過度複雜，參數超過了資料的個數，所以會完美的詮釋training dataset。

- Class: cmd_question
  Output: |
    請同學輸入：`p2.train <- predict(m2, df.train, type = "response")`

- Class: cmd_question
  Output: |
    接著我們輸入：`plot(Class ~ p2.train, df.train)`

- Class: text
  Output: |
    我們可以看到，如果切點在0.1的話，m2完美的將bad/good給切割開來了。
    
- Class: cmd_question
  Output: |
    我們再輸入`p2 <- predict(m2, df.test, type = "response")`

- Class: cmd_question
  Output: |
    我們再輸入`plot(Class ~ p2, df.test)`以視覺化的方式看看在df.test上的表現

- Class: text
  Output: |
    我們可以看到，預測的不同機率值下，bad/good的比率沒有明顯變化。顯示`p2`的分類
    結果很不好。
    
- Class: text
  Output: |
    我們也可以用一些數值指標來比較p1與p2的表現。一種常用的指標是Logarithmic Loss：
    - (y * log(p) + (1 - y) * log(1 - p)) 這裡的y是以0或1來代表分類結果。p則是預測
    發生1的機率。當完美預測（y = 1 時 p = 1, y = 0時p = 0），Logarithmic Loss的結果
    會趨近於0。當結果越不準確，Logarithmic Loss會越大
    
- Class: cmd_question
  Output: |
    透過R 的向量化運算，我們可以很快速的算出這個指標。請同學先輸入：`y <- df.test$Class == "good"`代表解答

- Class: cmd_question
  Output: |
    我們計算p1的Logarithmic Loss。請同學輸入：`-sum(y * log(p1) + (1 - y) * log(1-p1))`

- Class: cmd_question
  Output: |
    我們計算p2的Logarithmic Loss。請同學輸入：`-sum(y * log(p2) + (1 - y) * log(1-p2))`

- Class: text
  Output: |
    p1 的 Logarithmic Loss 遠小於 p2，代表它的預測準確率勝過p2。

- Class: text
  Output: |
    這是一個標準的overfitting的範例。m2在training dataset上完美的詮釋了類別的變化，但是在
    testing dataset上表現卻不如m1。這表示說，並不是描述training dataset越好，在其他資料上
    的表現就會越好。這個現象，讓機器學習的問題更具挑戰。

- Class: script
  Output: |
    事實上，我們可以選到更好的模型組合。請同學挑戰看看。
  Script: ml-02.R
  AnswerTests: ml_02()