- Class: meta
  Course: DataScienceAndR
  Lesson: Optional-RMachineLearning-03-Regularization
  Author: Wush Wu
  Type: Standard
  Organization: Taiwan R User Group
  Version: 2.3.1.2

- Class: text
  Output: |
    這個課程將跟同學介紹近代機器學習常用的一個技術。

- Class: text
  Output: |
    在之前的課程，我們是選擇解釋變數的組合、交互作用等方式，來挑選出好的模型。

- Class: text
  Output: |
    這堂課程介紹的作法，是一次放入所有可能的變數，然後透過修改「評分」項目，讓
    演算法來自動找出應該要留在模型裡的變數。這種方法，專業術語叫做：Regularization。

- Class: text
  Output: |
    R 的glmnet套件在線性模型上實作了Regularization的相關功能。這個套件的作者群，
    也是機器學習領域中鼎鼎大名的學者。

- Class: cmd_question
  Output: |
    請同學先安裝glmnet套件。
  CorrectAnswer: check_then_install("glmnet", "2.0.3")
  AnswerTests: test_package_version("glmnet", "2.0.3")

- Class: cmd_question
  Output: |
    接著請載入glmnet套件
  CorrectAnswer: library(glmnet)
  AnswerTests: test_search_path("glmnet")

- Class: cmd_question
  Output: |
    摸索套件的第一步：找尋vignettes。請同學輸入：`vignette(package = "glmnet")`

- Class: cmd_question
  Output: |
    請同學輸入`vignette("glmnet_beta", package = "glmnet")`打開套件的簡介文件。

- Class: text
  Output: |
    由於閱讀這份說明文件，需要統計學的專業知識，所以我們就先暫停在這邊。有興趣的
    同學可以再自行鑽研。

- Class: cmd_question
  Output: |
    這裡我們先用cars這個資料集來介紹glmnet與regularization。請同學先輸入：
    `m0 <- lm(Sepal.Length ~ ., iris)`這是第一個單元介紹的線性模型，透過這個指令，
    我們可以取得最小方差直線。我們等等會拿這個模型與glmnet的結果做比較。

- Class: cmd_question
  Output: |
    glmnet的參數並不接受formula，而是接受x(矩陣)與y(向量)。請同學先輸入：
    `X <- model.matrix(Sepal.Length ~ ., iris)`建立一個矩陣好填入glmnet的x參數。
    model.matrix是lm使用formula時，在背後產生線性代數矩陣的函數。

- Class: cmd_question
  Output: |
    接著，請同學輸入：`y <- iris$Sepal.Length`建立一個向量好填入glmnet的y參數。

- Class: cmd_question
  Output: |
    我們先輸入：`m <- glmnet(X[,-1], y, lambda = 0)`
    這裡使用X[,-1]是因為glmnet會自動加入Intercept，所以我們要把Intercept從X中移除
    （Intercept是第一欄）。關於lambda，我們先賣個關子。

- Class: cmd_question
  Output: |
    請同學輸入：`cbind(coef(m, s = 0), coef(m0))`比較一下glmnet學出來的模型與lm學出來的模型。
    coef的s = 0參數等等會補充說明。

- Class: text
  Output: |
    我們應該看到非常接近的數字。這是因為兩者要解的目標函數是一樣的，但是glmnet是用數值解，而
    lm是用理論直接計算，所以lm是比較準確的。

- Class: cmd_question
  Output: 接著，我們慢慢加大lambda參數。請同學出入：`m <- glmnet(X[,-1], y, lambda = seq(1, 0, by = -0.1))`

- Class: cmd_question
  Output: |
    請同學看看`m`的輸出結果。

- Class: text
  Output: |
    事實上，glmnet剛剛在背後一口氣算了時一個模型，分別對應到11個lambda（即seq(1.0, 0.0, by = -0.1)）

- Class: cmd_question
  Output: |
    我們可以運用`coef(m, s = 0.5)`來查詢當lambda = 0.5時，學出來的參數。請同學試試看

- Class: text
  Output: |
    同學應該可以注意到，除了Petal.Length之外，其他的參數學出來都是"."。這就是glmnet透過
    regularization的技術，去挑選變數。挑選的結果，在lambda = 0.5之下，只有挑出Petal.Length而已，
    其他的變數都是0。

- Class: cmd_question
  Output: |
    接著請同學輸入：`coef(m, s = 0.1)`

- Class: text
  Output: |
    這次同學可以看到，當lambda是0.1時，glmnet挑出的變數是Sepal.Width與Petal.Length。

- Class: text
  Output: |
    所以lambda到底是什麼呢？機器學習的原理，就是在訂定評分機制後，找到能夠最優化評分的參數。
    而regularization，就是在原本的評分之外，加上對參數數值的懲罰。具體作法是當參數的值越大，評分
    就會越低。而lambda就是控制懲罰強度的參數。

- Class: text
  Output: |
    舉例來說，當lambda = 0的時候，代表線性模型的參數值的懲罰加權為0（也就是不懲罰），所以學習
    出來的模型就等價於原本的線性模型。但是當lambda = 1時，參數值得懲罰讓機器學習最後把大部分
    的參數刪除，只留下Petal.Length。這就是regularization。

- Class: text
  Output: |
    glmnet所實作的regularization有兩種。一種是運用模型參數的絕對值的和做懲罰，這種又稱為L1-Regularization。
    統計上的別名是Lasso。另一種是以模型參數的平方的和作為懲罰，這種稱為L2-Regularization，統計上又稱為Ridge
    Regression。

- Class: text
  Output: |
    glmnet中的參數alpha控制了L1和L2 Regularization的權重。當alpha = 0，就只剩下L2-Regularization；
    當alpha = 1，就只剩下L1-Regularization。當alpha 介於0、1之間，兩者並存時，統計上稱作
    elastic net。

- Class: text
  Output: |
    回到選變數的問題。當我們使用預設的alpha = 1時，隨著lambda的加大或縮小，模型參數變成0的順序，
    常常被用來當成評估變數重要性。舉例來說，在`m`之中，我們可以看到Petal.Length可能是最重要的參數，
    接著才是Sepal.Width。
    
- Class: cmd_question
  Output: |
    我們回過頭解釋`m`的顯示結果。請同學輸入`m`
  CorrectAnswer: m

- Class: text
  Output: |
    `m`會顯示一個有三個欄位的矩陣。第一欄位（Df，Degree of Freedom的簡稱）就代表有多少個不為0的模型參數；第二欄位（%Dev）
    就代表這個模型在training dataset上解釋了多少比率的變化；第三個欄位（Lambda）則代表lambda的值，
    也就是Regularization的強度（懲罰模型參數的值的強度）。

- Class: cmd_question
  Output: |
    要注意的是，類似的選模只有在alpha > 0的時候才會有效果。當alpha = 0時，參數通常都不會是0。
    請同學試試看：`glmnet(X[,-1], y, alpha = 0, lambda = seq(1, 0, by = -0.1))`

- Class: text
  Output: |
    透過Df，同學可以發現所有的變數都不為0。

- Class: text
  Output: |
    類似glm，glmnet除了傳統的線性模型之外，也將類似的概念實作於廣義線性模型上。glmnet會偵測y的型態，
    或是指定family後，就會以對應的廣義線性模型做處理。我們直接在最後的關卡練習glmnet提供的logistic
    regression。

- Class: text
  Output: |
    實務上，glmnet這套作法雖然解決選模的問題，但是帶來挑選lambda的問題。像lambda這類參數，很難透過
    傳統的統計方法找到好的值。所以一般都會用training dataset v.s. testing dataset的概念來解決。
    也就是說，看看不同lambda選出來的模型，在testing dataset上的表現優劣，來挑選好的lambda。

- Class: script
  Output: |
    最後我們請同學用上一個單元的使用過得資料集合來作練習。
  Script: ml-03.R
  AnswerTests: ml_03()