#' 給定一個矩陣X，
X <- cbind(x1 = 1, x2 = 1:10, x3 = sin(1:10))
#' 以及一個長度為3 的向量 beta。
beta <- c(0.5, -1, 4.3)

#' 我們稱`X[,1]`為x1, `X[,2]`為x2, `X[,3]`為x3，
#' 向量y 的值是 x1 * beta[1] + x2 * beta[2] + x3 * beta[3]，
#' 請用矩陣乘法`%*%`算出向量y。
#'     dim(y) 應該是 c(10, 1)
y <- NULL # 請將NULL替換成你的程式碼。

#' epsilon 是一個隨機產生的雜訊向量，
epsilon <- c(-1.24462014500259, 0.146172987456978, 1.56426869006839, -0.856920339050681,
    -1.15277300953772, 0.717919832604741, -0.270623615316431, -1.66281578024014,
    -1.15557078461633, -0.730253254897595)

#' 我們讓y 參雜了雜訊。
y <- y + epsilon

#' 假設我們只知道X和y ，而看不到epsilon和beta。根據X 和y ，要如何找出beta?
#'
#' 這是一個標準的迴歸分析問題:
#'
#' 在握有X 和Y 等資料之後，尋找資料之間的線性關係（即beta）。
#'
#' 例如，尋找車速與煞車滑行距離之間的關係。
#'
#' 請參考<https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation>裡的公式：
#' $(X^T X)^{-1} X^T y$（方程式的圖片版： <http://i.imgur.com/Aykv7W3.png>），利用這章學到的矩陣乘法，與線性代數函式，算出beta的估計值。
#'
#' 你可以寫很多行的程式碼，但是請把結果存到beta.hat這個變數之中
#' ps. class(beta.hat) 應該要是 matrix
#'     dim(beta.hat) 應該是 c(3, 1)
#'     rownames(beta.hat) 應該是 c("x1", "x2", "x3")
#'
#' 對於沒有修過線性代數，或是未來沒有需要深入了解機器學習或統計相關演算法的同學，可以直接從網站上的參考答案取得答案，跳過這個練習
#' 網址: <http://datascienceandr.org/note/01-RBasic-05-Arrays-Matrices.html#-64>

beta.hat <- NULL # 請將NULL替換成你的程式碼

#' 同學可以比較一下beta.hat和beta，體驗看看迴歸分析的方法，是不是真的有道理。
